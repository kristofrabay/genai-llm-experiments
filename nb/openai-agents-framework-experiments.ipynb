{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing OpenAI's new Responses API and Agents SDK\n",
    "\n",
    "OpenAI announced on March 11th, 2025 new Responses API and Agents SDK (https://openai.com/index/new-tools-for-building-agents/) \n",
    "\n",
    "This notebook aims to test these new products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "#prev openai version: 1.59.8\n",
    "#new openai version: 1.66.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Responses API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-4o\"\n",
    "instruction = \"Write a one-sentence bedtime story about a unicorn.\"\n",
    "\n",
    "### ChatCompletions\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": instruction\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "### Responses API\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.0,\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": instruction\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion response: \n",
      "In a moonlit meadow, a gentle unicorn named Luna sprinkled stardust with her shimmering horn, lulling the entire forest into a peaceful, enchanted slumber.\n",
      "------------------------------------------------------------\n",
      "Responses response: \n",
      "Under the shimmering moonlight, Luna the unicorn gently trotted through the enchanted forest, her mane sparkling with stardust, as she whispered dreams of magic and wonder to all the sleeping creatures.\n"
     ]
    }
   ],
   "source": [
    "print(f'ChatCompletion response: \\n{completion.choices[0].message.content}')\n",
    "print('---' * 20)\n",
    "print(f'Responses response: \\n{response.output_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output_text == response.output[0].content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore new Responses output object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"o3-mini\"\n",
    "\n",
    "system_message = \"You are a witty joke writer. You have no political correctness filter. You're number one goal is to tell jokes without being afraid of offending anyone\"\n",
    "\n",
    "topic = \"us-canada geopolitical relations\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=None if 'o3' in MODEL_NAME or 'o1' in MODEL_NAME else 0.0,\n",
    "    reasoning={\n",
    "        \"effort\": \"high\",\n",
    "        #\"generate_summary\" : \"concise\" # detailed\n",
    "        },\n",
    "    instructions=system_message,\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Given the user's preferred topic, generate a joke. User's topic: {topic}\"\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the U.S. call off its plans to invade Canada? Because every time a missile was about to launch, Ottawa simply shouted, \"Sorry, eh?\"—proving that a barrage of polite apologies is the ultimate weapon of mass deterrence!\n"
     ]
    }
   ],
   "source": [
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 83\n",
      "Reasoning tokens: 3264\n",
      "Net output tokens: 52\n",
      "Output tokens: 3316\n"
     ]
    }
   ],
   "source": [
    "input_tokens = response.usage.input_tokens\n",
    "output_tokens = response.usage.output_tokens\n",
    "reasoning_tokens = response.usage.output_tokens_details.reasoning_tokens\n",
    "net_output_tokens = output_tokens - reasoning_tokens\n",
    "\n",
    "print(f'Input tokens: {input_tokens}')\n",
    "print(f'Reasoning tokens: {reasoning_tokens}')\n",
    "print(f'Net output tokens: {net_output_tokens}')\n",
    "print(f'Output tokens: {output_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reasoning(effort='high', generate_summary=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_67d1cdee27a08192beb956158fb8a02d04bbb1bb73a141e7', summary=[], type='reasoning', status=None),\n",
       " ResponseOutputMessage(id='msg_67d1cdf354348192a050188302163acc04bbb1bb73a141e7', content=[ResponseOutputText(annotations=[], text='Why did the U.S. call off its plans to invade Canada? Because every time a missile was about to launch, Ottawa simply shouted, \"Sorry, eh?\"—proving that a barrage of polite apologies is the ultimate weapon of mass deterrence!', type='output_text')], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the U.S. call off its plans to invade Canada? Because every time a missile was about to launch, Ottawa simply shouted, \"Sorry, eh?\"—proving that a barrage of polite apologies is the ultimate weapon of mass deterrence!'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output[-1].content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured output\n",
    "\n",
    "The direct BaseModel passing is still in beta, so will convert class to JSON schema\n",
    "\n",
    "https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses&example=structured-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    \"\"\"A joke about the user's preferred topic\"\"\"\n",
    "\n",
    "    joke: str = Field(description=\"A joke about the user's preferred topic\")\n",
    "    explanation: str = Field(description=\"An explanation of the joke\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_name(cls):\n",
    "        return \"joke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_format = {\n",
    "    \"format\": {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"name\": Joke.get_name(),\n",
    "        \"schema\": {\n",
    "            **Joke.model_json_schema(),\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=None if 'o3' in MODEL_NAME or 'o1' in MODEL_NAME else 0.0,\n",
    "    reasoning={\n",
    "        \"effort\": \"high\",\n",
    "        #\"generate_summary\" : \"concise\" # detailed\n",
    "        },\n",
    "    \n",
    "    instructions=system_message,\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Given the user's preferred topic, generate a joke. User's topic: {topic}\"\n",
    "        }\n",
    "    ],\n",
    "    text=structured_format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': \"At the latest summit, the US declared, 'We need a wall to keep out our problems,' while Canada just smiled and said, 'Sorry, eh? How about we resolve them over some hockey and maple syrup instead?'\", 'explanation': 'This joke plays on the classic stereotypes: the US with its bold, sometimes confrontational policies (like building walls), and Canada with its trademark politeness and laid-back charm, suggesting that even geopolitical disputes could be smoothed over with a friendly game of hockey and a taste of maple syrup.'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response_dict = json.loads(response.output[-1].content[0].text)\n",
    "except json.JSONDecodeError as e:\n",
    "    response_dict = response.output[-1].content[0].text\n",
    "\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaways:\n",
    "\n",
    "- system_message can be passed as `instructions` parameter or as a separate `system` message in the `input` parameter.\n",
    "- there is a `generate_summary` parameter that could be used to show concise or detailed reasoning - but does not work yet with any of the reasoning models.\n",
    "- no direct Structured Output support, but `json_schema` in `text` field works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web search tool\n",
    "\n",
    "https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses\n",
    "\n",
    "Available values for `search_context_size`:\n",
    "\n",
    "- high: Most comprehensive context, highest cost, slower response.\n",
    "- medium (default): Balanced context, cost, and latency.\n",
    "- low: Least context, lowest cost, fastest response, but potentially lower answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'gpt-4o'\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=None if 'o3' in MODEL_NAME or 'o1' in MODEL_NAME else 0.0,\n",
    "    instructions=system_message,\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Given the user's preferred topic, generate a joke. User's topic: {topic}\"\n",
    "        }\n",
    "    ],\n",
    "    tools=[{\n",
    "        \"type\": \"web_search_preview\",\n",
    "        \"search_context_size\": \"low\",\n",
    "    }],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
