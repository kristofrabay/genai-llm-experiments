{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from utils.llm import chat_completion, achat_completion, format_response\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = OpenAI()\n",
    "aclient = AsyncOpenAI()\n",
    "pplx_client = OpenAI(api_key=os.getenv('PPLX_API_KEY'), base_url=\"https://api.perplexity.ai\")\n",
    "\n",
    "anthropic_client = Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Synchronous Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4o\"\n",
    "o1_model_name = 'o1-preview'\n",
    "model_name_pplx = \"llama-3.1-sonar-huge-128k-online\"\n",
    "anthropic_model_name = \"claude-3-5-sonnet-20241022\"\n",
    "temperature = 0.0\n",
    "\n",
    "system_message = \"You are a helpful assistant to an experienced data scientist.\"\n",
    "user_message = \"Using a table, explain the difference between LightGBM, XGBoost, and CatBoost.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ],\n",
    "    model_name=model_name,\n",
    "    temperature=temperature,\n",
    "    client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Usage: {'completion_tokens': 491, 'prompt_tokens': 42, 'total_tokens': 533, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Response: Certainly! Below is a table that outlines the key differences between LightGBM, XGBoost, and CatBoost, which are all popular gradient boosting frameworks:\n",
       "\n",
       "| Feature/Aspect       | LightGBM                                      | XGBoost                                      | CatBoost                                      |\n",
       "|----------------------|-----------------------------------------------|----------------------------------------------|-----------------------------------------------|\n",
       "| **Developer**        | Microsoft                                     | Distributed Machine Learning Community       | Yandex                                        |\n",
       "| **Language Support** | C++, Python, R, Java, C#, and more            | C++, Python, R, Java, Scala, and more        | Python, R, Java, C++, and more                |\n",
       "| **Tree Structure**   | Leaf-wise growth with depth limitation        | Level-wise growth                            | Symmetric trees                               |\n",
       "| **Handling Categorical Features** | Requires preprocessing (e.g., one-hot encoding) | Requires preprocessing (e.g., one-hot encoding) | Native support for categorical features       |\n",
       "| **Speed**            | Generally faster due to leaf-wise growth      | Slower compared to LightGBM and CatBoost     | Fast, especially with categorical features    |\n",
       "| **Memory Usage**     | Lower memory usage due to efficient algorithms| Higher memory usage                          | Efficient memory usage                        |\n",
       "| **Parallelization**  | Efficient parallelization                     | Good parallelization                         | Efficient parallelization                     |\n",
       "| **Overfitting Control** | Can overfit if not tuned properly          | Good control with regularization parameters  | Good control with built-in regularization     |\n",
       "| **Missing Values**   | Handles missing values natively               | Handles missing values natively              | Handles missing values natively               |\n",
       "| **GPU Support**      | Yes                                           | Yes                                          | Yes                                           |\n",
       "| **Ease of Use**      | Requires careful tuning for optimal performance| Well-documented and widely used              | User-friendly with automatic handling of categorical features |\n",
       "| **Use Cases**        | Large datasets, high-dimensional data         | General-purpose, tabular data                | Datasets with categorical features, time series|\n",
       "\n",
       "Each of these frameworks has its strengths and is suited to different types of problems. LightGBM is known for its speed and efficiency, especially with large datasets. XGBoost is a robust and widely-used framework with a strong community and extensive documentation. CatBoost excels in handling categorical features and is user-friendly, making it a great choice for datasets with many categorical variables."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response, usage = format_response(response)\n",
    "\n",
    "display(Markdown(f'Usage: {usage}'))\n",
    "display(Markdown(f'Response: {response}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI - reasoning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Usage: {'completion_tokens': 1909, 'prompt_tokens': 49, 'total_tokens': 1958, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 960, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Response: Here is a table comparing LightGBM, XGBoost, and CatBoost:\n",
       "\n",
       "| Feature                      | LightGBM                                                    | XGBoost                                                    | CatBoost                                                    |\n",
       "|------------------------------|-------------------------------------------------------------|------------------------------------------------------------|-------------------------------------------------------------|\n",
       "| **Developer/Organization**   | Microsoft                                                   | Distributed (Deep) Machine Learning Community              | Yandex                                                      |\n",
       "| **Primary Language(s)**      | C++, with APIs for Python, R, etc.                          | C++, with APIs for Python, R, Java, Scala, etc.            | C++, with APIs for Python, R, etc.                          |\n",
       "| **Key Features**             | - Exclusive Feature Bundling<br>- Gradient-based One-Side Sampling (GOSS)| - Regularization techniques<br>- Parallel processing<br>- Weighted quantile sketch| - Native handling of categorical features<br>- Ordered boosting to reduce overfitting<br>- Support for text data|\n",
       "| **Speed and Performance**    | - Optimized for speed and memory efficiency<br>- Generally faster on large datasets| - High performance with optimized computing power<br>- Efficient with smaller to medium datasets| - Competitive speed, especially with categorical features<br>- Efficient with default parameter settings|\n",
       "| **Handling of Categorical Features** | - Supports categorical features by converting them to numerical values internally<br>- May require tuning for optimal results| - Does not natively handle categorical features<br>- Requires preprocessing like one-hot encoding| - Excellent native handling of categorical features<br>- No need for preprocessing|\n",
       "| **GPU Support**              | Yes                                                        | Yes                                                       | Yes                                                        |\n",
       "| **Tree Growth Algorithm**    | - Leaf-wise (best-first) growth<br>- Can result in deeper trees and potential overfitting if not regularized| - Level-wise (depth-wise) growth<br>- Balances tree depth and can be more stable| - Symmetric tree growth<br>- Builds balanced trees, which can prevent overfitting|\n",
       "| **Missing Value Handling**   | - Automatically handles missing values<br>- Learns optimal splits for missing values| - Automatically handles missing values<br>- Assigns default direction in the tree| - Automatically handles missing values<br>- Considers missing values as a separate category|\n",
       "| **Regularization Techniques**| - Supports L1 and L2 regularization<br>- Provides parameters to control tree complexity| - Supports L1 and L2 regularization<br>- Parameters for tree complexity control<br>- Supports monotonic constraints| - Uses overfitting detector<br>- Supports L2 leaf regularization<br>- Incorporates various parameters to prevent overfitting|\n",
       "| **Support for Sparse Data**  | - Efficient handling of sparse data<br>- Sparse optimization for features| - Efficient handling of sparse data<br>- Specialized data structures for sparsity| - Supports sparse data<br>- Handles missing and zero values effectively|\n",
       "| **Ease of Use and API**      | - User-friendly APIs<br>- Similar interface to XGBoost<br>- Flexible with many parameters| - User-friendly APIs<br>- Widely adopted with extensive examples<br>- Supports multiple languages| - User-friendly APIs<br>- Simplifies handling of categorical data<br>- Less need for parameter tuning|\n",
       "| **Documentation and Community Support** | - Good documentation<br>- Growing community<br>- Active development| - Extensive documentation<br>- Large community<br>- Widely used in competitions| - Comprehensive documentation<br>- Growing community support<br>- Active development|\n",
       "| **Use Cases**                | - Ideal for large datasets<br>- High-dimensional data<br>- When speed is critical| - General-purpose model<br>- Suitable for a wide range of tasks<br>- Often used as a benchmark| - Datasets with categorical features<br>- When ease of use is important<br>- Tasks with text or categorical data|\n",
       "\n",
       "**Summary:**\n",
       "\n",
       "- **LightGBM** is optimized for speed and performance, especially with large datasets. It uses a leaf-wise tree growth strategy that can result in deeper trees and potential overfitting if not properly regularized. It provides support for categorical features and missing values but may require parameter tuning for optimal results.\n",
       "\n",
       "- **XGBoost** is known for its flexibility and performance, providing extensive options for regularization and tree control. It does not natively handle categorical features, requiring preprocessing steps like one-hot encoding. XGBoost is widely used and has a large community, making it a reliable choice for many machine learning tasks.\n",
       "\n",
       "- **CatBoost** excels in handling categorical features natively without the need for preprocessing. It uses ordered boosting and symmetric trees to combat overfitting and improve prediction accuracy. CatBoost is user-friendly with less parameter tuning required, making it suitable for datasets with a large number of categorical variables."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_o1 = chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ],\n",
    "    model_name=o1_model_name,\n",
    "    temperature=temperature,\n",
    "    client=client\n",
    ")\n",
    "response_o1, usage_o1 = format_response(response_o1)\n",
    "\n",
    "display(Markdown(f'Usage: {usage_o1}'))\n",
    "display(Markdown(f'Response: {response_o1}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_pplx = chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ],\n",
    "    model_name=model_name_pplx,\n",
    "    temperature=temperature,\n",
    "    client=pplx_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Usage: {'completion_tokens': 493, 'prompt_tokens': 31, 'total_tokens': 524, 'completion_tokens_details': None, 'prompt_tokens_details': None}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Response: **Comparison of LightGBM, XGBoost, and CatBoost**\n",
       "\n",
       "| **Feature** | **LightGBM** | **XGBoost** | **CatBoost** |\n",
       "| --- | --- | --- | --- |\n",
       "| **Tree Growth Strategy** | Leaf-wise (depth-first) | Level-wise (breadth-first) | Symmetric (balanced trees) |\n",
       "| **Handling Categorical Features** | Native support with GOSS and EFB | Native support with one-hot encoding | Specifically designed for categorical features |\n",
       "| **Handling Missing Values** | Native support | Native support | Native support |\n",
       "| **Parallelization** | Multi-threaded with OpenMP | Multi-threaded with OpenMP | Single-threaded by default, but can be parallelized |\n",
       "| **GPU Support** | Yes | Yes | Yes |\n",
       "| **Prediction Latency** | Generally slower than CatBoost | Generally slower than CatBoost | Fastest in many cases, especially with high concurrent load |\n",
       "| **Training Time** | Can be faster than CatBoost with GOSS and EFB | Can be slower than LightGBM | Can be slower than LightGBM and XGBoost |\n",
       "| **Accuracy** | High accuracy, especially with GOSS and EFB | High accuracy | High accuracy, especially with categorical features |\n",
       "| **Scalability** | Scalable with large datasets | Scalable with large datasets | Scalable with large datasets |\n",
       "\n",
       "**Key Points:**\n",
       "- **LightGBM** excels with its leaf-wise tree growth strategy and native support for categorical features and missing values. It is particularly fast with GOSS and EFB.\n",
       "- **XGBoost** uses a level-wise tree growth strategy and supports one-hot encoding for categorical features. It is known for its regularization techniques and high accuracy.\n",
       "- **CatBoost** is specifically designed for handling categorical features and uses symmetric tree growth. It often has the fastest prediction latency, especially under high concurrent loads.\n",
       "\n",
       "**References:**\n",
       "- [1] https://tech.deliveryhero.com/is-catboost-faster-than-lightgbm-and-xgboost/\n",
       "- [2] https://arxiv.org/pdf/2305.17094.pdf\n",
       "- [4] https://www.geeksforgeeks.org/gradientboosting-vs-adaboost-vs-xgboost-vs-catboost-vs-lightgbm/\n",
       "- [5] https://datamapu.com/posts/classical_ml/gradient_boosting_variants/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Citations: ['https://tech.deliveryhero.com/is-catboost-faster-than-lightgbm-and-xgboost/', 'https://arxiv.org/pdf/2305.17094.pdf', 'https://www.youtube.com/watch?v=R5FB1ZUejXM', 'https://www.geeksforgeeks.org/gradientboosting-vs-adaboost-vs-xgboost-vs-catboost-vs-lightgbm/', 'https://datamapu.com/posts/classical_ml/gradient_boosting_variants/']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_content_pplx, usage_pplx = format_response(response_pplx)  \n",
    "\n",
    "display(Markdown(f'Usage: {usage_pplx}'))\n",
    "display(Markdown(f'Response: {response_content_pplx}'))\n",
    "display(Markdown(f'Citations: {response_pplx.citations}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_anthropic = anthropic_client.messages.create(\n",
    "    model=anthropic_model_name,\n",
    "    max_tokens=8192,\n",
    "    temperature=temperature,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_message\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a comparative table explaining the key differences between LightGBM, XGBoost, and CatBoost:\n",
       "\n",
       "| Feature | LightGBM | XGBoost | CatBoost |\n",
       "|---------|----------|----------|-----------|\n",
       "| **Development** | By Microsoft | By DMLC (Distributed Machine Learning Community) | By Yandex |\n",
       "| **Tree Growth** | Leaf-wise (best-first) growth | Level-wise growth | Symmetric tree growth |\n",
       "| **Speed** | Fastest among the three | Moderate speed | Moderate to fast |\n",
       "| **Memory Usage** | Most memory efficient | Higher memory usage | Moderate memory usage |\n",
       "| **Handling Categorical Features** | Native support with optimal binning | No native support (requires pre-encoding) | Best native support with several encoding methods |\n",
       "| **Missing Values** | Native handling | Native handling | Native handling |\n",
       "| **Key Advantage** | Speed and efficiency | Accuracy and wide adoption | Handling categorical features and reducing overfitting |\n",
       "| **Best Use Case** | Large datasets with limited resources | General-purpose, smaller to medium datasets | Datasets with many categorical features |\n",
       "| **Hyperparameter Tuning** | More parameters, requires careful tuning | Many parameters, well-documented | Fewer parameters, easier to tune |\n",
       "| **Risk of Overfitting** | Higher (especially with small datasets) | Moderate | Lower (built-in mechanisms to prevent overfitting) |\n",
       "| **GPU Support** | Yes | Yes | Yes |\n",
       "| **Parallel Processing** | Yes | Yes | Yes |\n",
       "| **Learning Task Support** | Classification, Regression, Ranking | Classification, Regression, Ranking | Classification, Regression |\n",
       "| **Community & Documentation** | Growing community, good documentation | Largest community, excellent documentation | Smaller community, good documentation |\n",
       "| **Production Readiness** | Good | Excellent | Good |\n",
       "\n",
       "Note: The performance of each algorithm can vary significantly depending on the specific use case, data characteristics, and implementation details."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response_anthropic.content[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_anthropic.content[0].citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=43, output_tokens=461)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_anthropic.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Asynchronous Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_async = await achat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ],\n",
    "    model_name=model_name,\n",
    "    temperature=temperature,\n",
    "    client=aclient\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Usage: {'completion_tokens': 600, 'prompt_tokens': 42, 'total_tokens': 642, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Response: Certainly! Below is a table that outlines the key differences between LightGBM, XGBoost, and CatBoost, which are all popular gradient boosting frameworks:\n",
       "\n",
       "| Feature/Aspect       | LightGBM                                      | XGBoost                                      | CatBoost                                      |\n",
       "|----------------------|-----------------------------------------------|----------------------------------------------|-----------------------------------------------|\n",
       "| **Developer**        | Microsoft                                     | Distributed Machine Learning Community       | Yandex                                        |\n",
       "| **Language**         | C++ (with interfaces for Python, R, etc.)     | C++ (with interfaces for Python, R, etc.)    | C++ (with interfaces for Python, R, etc.)     |\n",
       "| **Tree Structure**   | Leaf-wise growth with depth limitation        | Level-wise growth                            | Symmetric tree (balanced)                     |\n",
       "| **Handling Categorical Features** | Requires preprocessing (e.g., one-hot encoding) | Requires preprocessing (e.g., one-hot encoding) | Native support for categorical features       |\n",
       "| **Speed**            | Generally faster due to leaf-wise growth      | Slower compared to LightGBM and CatBoost     | Fast, especially with categorical features    |\n",
       "| **Memory Usage**     | Lower memory usage due to efficient algorithms| Higher memory usage                          | Efficient memory usage                        |\n",
       "| **Parallelization**  | Efficient parallelization                     | Good parallelization                         | Efficient parallelization                     |\n",
       "| **Overfitting Control** | Can overfit if not properly tuned          | Good control with regularization parameters  | Good control with built-in regularization     |\n",
       "| **Missing Values**   | Handles missing values natively               | Handles missing values natively              | Handles missing values natively               |\n",
       "| **GPU Support**      | Yes                                           | Yes                                          | Yes                                           |\n",
       "| **Ease of Use**      | Requires careful tuning for best performance  | Well-documented and widely used              | User-friendly with automatic handling of categorical features |\n",
       "| **Use Cases**        | Large datasets, high-dimensional data         | General-purpose, widely applicable           | Datasets with categorical features, ease of use |\n",
       "\n",
       "### Additional Notes:\n",
       "- **LightGBM**: Known for its speed and efficiency, especially with large datasets. The leaf-wise growth strategy can lead to faster convergence but may require careful tuning to avoid overfitting.\n",
       "- **XGBoost**: One of the most popular and widely used gradient boosting libraries. It is known for its robust performance and flexibility, with a wide range of parameters for fine-tuning.\n",
       "- **CatBoost**: Particularly strong in handling categorical features without the need for extensive preprocessing. It is designed to be user-friendly and often requires less parameter tuning compared to the other two.\n",
       "\n",
       "Each of these frameworks has its strengths and is suitable for different types of problems and datasets. The choice between them often depends on the specific requirements of the task, such as the size of the dataset, the presence of categorical features, and the need for speed versus interpretability."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_async, usage_async = format_response(response_async)\n",
    "\n",
    "display(Markdown(f'Usage: {usage_async}'))\n",
    "display(Markdown(f'Response: {response_async}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
